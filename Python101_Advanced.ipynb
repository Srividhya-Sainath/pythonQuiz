{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXljZDhE2-r9"
      },
      "source": [
        "\n",
        "Attention, all aspiring Python learners and coding clinicians! Welcome to our Python Quiz (Level - Advanced)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy_r37Zp7olR"
      },
      "source": [
        "Prereq: you need a Google account\n",
        "\n",
        "Step 1. Download the sample image from: https://drive.google.com/file/d/1zQWqchhH5tqZz2lwdAlJFONCvVvWKnDY/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLesGHss7x0k"
      },
      "source": [
        "Step 2. Upload the sample image, \"cat_dog.jpg\" into the panel on the left"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPewbsmfCbhp"
      },
      "source": [
        "**Important:** the packages below are ALL you need to complete the exercises. You'll be asked to use these packages, but you can of course add whatever you deem necessary. What differentiates a **beginning programmer vs. an experienced programmer**, is the ability to Google problems and error messages effectively. If you don't know how to do something, **just Google it**, read the documentation of the packages, and see if you know what needs to happen and how it happens.\n",
        "\n",
        "Only once you're an experienced programmer, you can use tools like ChatGPT to accelerate your development. However, we do NOT recommend using ChatGPT if you haven't mastered programming itself, otherwise you will not be able to detect subtle (but devastating!) mistakes that are being made in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install numpy matplotlib scikit-learn opencv-python-headless pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5R5uzqF2YLL"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import sklearn\n",
        "import cv2 # opencv\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhI02e7p6gCZ"
      },
      "source": [
        "# **Exercise 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "aJ0vqCGm9QOi",
        "outputId": "1ac43817-eee6-4834-a4e8-c1a0c10c0461"
      },
      "outputs": [],
      "source": [
        "# Load the image and show it\n",
        "img = Image.open('cat_dog.jpg')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_yDsYpr9UOf",
        "outputId": "67f771dc-4867-4032-c132-33cadd2d9ce5"
      },
      "outputs": [],
      "source": [
        "# Crop the image so you have 2 groups of animals - 1 cat and 1 dog\n",
        "img_array = np.array(img)\n",
        "print(img_array.shape)\n",
        "animal_a = img_array[70:230, 80:230, :]\n",
        "animal_b = img_array[280:400, 520:, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "odrkqEhVBCEc",
        "outputId": "30a84a9f-3947-4b31-ec96-a0e850d91361"
      },
      "outputs": [],
      "source": [
        "# Visualize so you know you have the correct crop for the dog\n",
        "plt.imshow(animal_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "G57RG1UbBDYk",
        "outputId": "078d2c3a-7eec-4c7e-dbf1-a81e8faa3e1b"
      },
      "outputs": [],
      "source": [
        "# Visualize so you know you have the correct crop for the cat\n",
        "plt.imshow(animal_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvdRJpIYBMnF"
      },
      "outputs": [],
      "source": [
        "# Save the cat and dog image as two new files\n",
        "Image.fromarray(animal_a).save('animal_b_crop.jpg')\n",
        "Image.fromarray(animal_b).save('animal_a_crop.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkhnxHDp7JJP"
      },
      "source": [
        "# **Exercise 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Œ Why Use `cv2` (OpenCV)?\n",
        "\n",
        "1. **OpenCV (`cv2`)** is a fast and optimized library for image and video processing.\n",
        "2. Supports **JPEG, PNG, BMP, TIFF, RAW**, etc.\n",
        "3. Can perform **face detection, object tracking, image segmentation, edge detection**, etc.\n",
        "4. Used in deep learning frameworks like **TensorFlow & PyTorch** for vision tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Œ Why Convert BGR to RGB in OpenCV?\n",
        "\n",
        "1. When you load an image with `cv2.imread()`, OpenCV stores it in **BGR format** (Blue-Green-Red) instead of the usual **RGB format** (Red-Green-Blue).  \n",
        "\n",
        "2. Libraries like **Matplotlib, PIL, and TensorFlow** expect **RGB images**, so colors will appear incorrectly if not converted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !!! Execute this Codeblock\n",
        "def show_image(image):\n",
        "    \"\"\"Converting BGR to RGB\"\"\"\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR -> RGB\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")  # Let's hide the axis for a clean look\n",
        "    plt.show()  # Show the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "txvpqMFf4njs",
        "outputId": "7095fdd9-e030-4025-f62d-1a69f4f57ac6"
      },
      "outputs": [],
      "source": [
        "# Read image and show it using cv2_imshow\n",
        "image_a = cv2.imread('animal_a_crop.jpg')\n",
        "show_image(image_a)\n",
        "\n",
        "# Optionally, convert the colored image from RGB to grayscale\n",
        "# and blur the image. This is computationally more efficient.\n",
        "gray_a = cv2.cvtColor(image_a, cv2.COLOR_BGR2GRAY)\n",
        "blurred_a = cv2.GaussianBlur(gray_a, (5, 5), 0)\n",
        "\n",
        "# Perform Edge Detection (using Canny edge detection) and show the image\n",
        "edges_a = cv2.Canny(blurred_a, threshold1=30, threshold2=100)  # You can adjust these thresholds as needed\n",
        "show_image(edges_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "LNqj4iCtMOWy",
        "outputId": "a45f97a7-df75-4506-b4c7-0baab0180b06"
      },
      "outputs": [],
      "source": [
        "# Read image and show it using cv2_imshow\n",
        "image_b = cv2.imread('animal_b_crop.jpg')\n",
        "show_image(image_b)\n",
        "\n",
        "# Optionally, convert the colored image from RGB to grayscale\n",
        "# and blur the image. This is computationally more efficient.\n",
        "gray_b = cv2.cvtColor(image_b, cv2.COLOR_BGR2GRAY)\n",
        "blurred_b = cv2.GaussianBlur(gray_b, (5, 5), 0)\n",
        "\n",
        "# Perform Edge Detection (using Canny edge detection) and show the image\n",
        "edges_b = cv2.Canny(blurred_b, threshold1=30, threshold2=100)  # You can adjust these thresholds as needed\n",
        "show_image(edges_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHF_qsHD7l9s"
      },
      "source": [
        "# **Exercise 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWPPX7WC8gXZ",
        "outputId": "da293b9d-00fe-4adc-8bb6-980df135ac47"
      },
      "outputs": [],
      "source": [
        "# calculate the number of contours in the image containing the edges (black/white) for both images\n",
        "# and print the length of the array to find how many edges are found\n",
        "contours_a, _ = cv2.findContours(edges_a.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "print(len(contours_a))\n",
        "\n",
        "contours_b, _ = cv2.findContours(edges_b.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "print(len(contours_b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yoLEmJoOcT6"
      },
      "outputs": [],
      "source": [
        "# repeat the above steps, cropping the dogs and cats and calculating the edges\n",
        "# for each of the animals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsiEAf_rNoW8"
      },
      "outputs": [],
      "source": [
        "# Is there a numerical difference between the number of edges in cats vs. dogs?\n",
        "# Plot a scatterplot and color the points based on their label (cat or dog)\n",
        "# Do you see a certain pattern?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYazLuZrQn21"
      },
      "source": [
        "# Bonus: Exercise 4\n",
        "\n",
        "Vanilla MLP classifier on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTtrL1BvaKGk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0s5Q_RGvaNdR"
      },
      "outputs": [],
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1Hed2k_aZph"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 224 * 224  # Input image size after resizing\n",
        "hidden_size = 100\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./sample_data', train=True, transform=transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "]), download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./sample_data', train=False, transform=transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "]))\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model\n",
        "model = MLP(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "927e3a6927c64904b53a13e474e4a5bb",
            "ba9175fdeee644c1a6a0d4dd6b372a1f",
            "7236da18f1524fecb487046d57adfe17",
            "cccdd1d2d4b0434b84395dba1315cbb7",
            "c7544455d4d449a68806b4e87a8fbefb",
            "2524ab0208ec4334a8ae91d853bbc9f8",
            "279bddbb99d64e079e728207bf439700",
            "fe1e1f9682c84145a00012e6e4cdc130",
            "caa06e580c6a450ea6350f9bfc5d04c5",
            "5ac97dc719d7485090118c4fddd7ff52",
            "01bb0b0389ce4ce9b1b05870ab502654"
          ]
        },
        "id": "mK4dCypBaWjh",
        "outputId": "d506570a-d180-46d6-e2a6-159d7aac742a"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "    for i, (images, labels) in loop:\n",
        "        images = images.reshape(-1, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.reshape(-1, input_size).to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy after epoch {epoch+1}: {100 * correct / total:.2f}%\")\n",
        "\n",
        "    # Switch back to training mode\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mRkbMrCwbeXy",
        "outputId": "c05b1480-4181-4fdc-a680-54e32809d9c0"
      },
      "outputs": [],
      "source": [
        "# Function to display images along with predictions\n",
        "def imshow(img, title):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Test the model on a few images\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images.reshape(-1, 224 * 224))  # Reshape images to match the input size\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        # Display the images and predictions of misclassifications\n",
        "        for i in range(len(images)):\n",
        "            if predicted[i] != labels[i]:\n",
        "              imshow(images[i].cpu(), title=f'Predicted: {predicted[i]}, Actual: {labels[i]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BggpOasIeMhP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01bb0b0389ce4ce9b1b05870ab502654": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2524ab0208ec4334a8ae91d853bbc9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279bddbb99d64e079e728207bf439700": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac97dc719d7485090118c4fddd7ff52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7236da18f1524fecb487046d57adfe17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe1e1f9682c84145a00012e6e4cdc130",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caa06e580c6a450ea6350f9bfc5d04c5",
            "value": 600
          }
        },
        "927e3a6927c64904b53a13e474e4a5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba9175fdeee644c1a6a0d4dd6b372a1f",
              "IPY_MODEL_7236da18f1524fecb487046d57adfe17",
              "IPY_MODEL_cccdd1d2d4b0434b84395dba1315cbb7"
            ],
            "layout": "IPY_MODEL_c7544455d4d449a68806b4e87a8fbefb"
          }
        },
        "ba9175fdeee644c1a6a0d4dd6b372a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2524ab0208ec4334a8ae91d853bbc9f8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_279bddbb99d64e079e728207bf439700",
            "value": "Epochâ€‡[1/1],â€‡Loss:â€‡0.1303:â€‡100%"
          }
        },
        "c7544455d4d449a68806b4e87a8fbefb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "caa06e580c6a450ea6350f9bfc5d04c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cccdd1d2d4b0434b84395dba1315cbb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ac97dc719d7485090118c4fddd7ff52",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_01bb0b0389ce4ce9b1b05870ab502654",
            "value": "â€‡600/600â€‡[01:35&lt;00:00,â€‡â€‡7.02it/s]"
          }
        },
        "fe1e1f9682c84145a00012e6e4cdc130": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
